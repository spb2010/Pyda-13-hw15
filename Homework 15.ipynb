{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: - - .\n",
    "\n",
    "Дополнительная часть (необязательная)\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: - - - <текст статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nНебольшая коллекция практик, трюков и подска...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nЗдравствуйте, это вторая статья о NGFW решен...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nПривет, Хабр, я член Brmlab Hackerspace в Пр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  \\nНебольшая коллекция практик, трюков и подска...\n",
       "1  \\nЗдравствуйте, это вторая статья о NGFW решен...\n",
       "2  \\nПривет, Хабр, я член Brmlab Hackerspace в Пр..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим пустой список новостей\n",
    "news_list = list()\n",
    "# получаем страницу с самыми свежими постами\n",
    "req = requests.get('https://habr.com/ru/all/')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "#будем вычленять части 'div', class_='post__text post__text-html post__text_v2'\n",
    "posts = soup.find_all('div', class_='post__text post__text-html post__text_v2')\n",
    "\n",
    "#ключевые слова\n",
    "#KEYWORDS = ['python', 'парсинг']\n",
    "#Ничего не находит, на всякий случай найдем другие слова\n",
    "KEYWORDS = ['статья', 'приложение', 'при']\n",
    "\n",
    "# извлекаем посты и записываем в список news_list\n",
    "for post in posts:\n",
    "    for words in KEYWORDS:\n",
    "        if words in post.text:\n",
    "            news_list.append(post.text)\n",
    "            \n",
    "#делаем датафрейм из списка\n",
    "habr_news = pd.DataFrame(news_list)\n",
    "habr_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "Обязательная часть\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: - <дата утечки> - <источник утечки> - <описание утечки>.\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'identityprotection.avast.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = {\n",
    "    'Vaar-Version': '0',\n",
    "    'Vaar-Header-App-Product': 'hackcheck-web-avast'\n",
    "  }\n",
    "#data = {'emailAddresses': ['xxx@x.ru', 'lol@lol.ru']}\n",
    "data = {'emailAddresses': ['spb2010@mail.ru', 'lefso@bk.ru']}\n",
    "json_data = json.dumps(data)\n",
    "response = requests.post(url, data=json_data, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>site</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-07T00:00:00Z</td>\n",
       "      <td>armorgames.com</td>\n",
       "      <td>In December 2018, Armor Games' database was al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-21T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>At some time in 2020, the Russian social netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-20T00:00:00Z</td>\n",
       "      <td>couponmom.com / armor games</td>\n",
       "      <td>In 2014, this list of records surfaced online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-23T00:00:00Z</td>\n",
       "      <td>kotofoto.ru</td>\n",
       "      <td>In November 2019, the Russian online electroni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-19T00:00:00Z</td>\n",
       "      <td>storelp.ru</td>\n",
       "      <td>At an unconfirmed date, StoreLP.ru's database ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-07-16T00:00:00Z</td>\n",
       "      <td>unknown</td>\n",
       "      <td>At an unconfirmed date, data purportedly assoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                         site  \\\n",
       "0  2019-03-07T00:00:00Z               armorgames.com   \n",
       "1  2020-05-21T00:00:00Z                       vk.com   \n",
       "2  2018-01-20T00:00:00Z  couponmom.com / armor games   \n",
       "3  2020-01-23T00:00:00Z                  kotofoto.ru   \n",
       "4  2018-06-19T00:00:00Z                   storelp.ru   \n",
       "5  2020-07-16T00:00:00Z                      unknown   \n",
       "\n",
       "                                         description  \n",
       "0  In December 2018, Armor Games' database was al...  \n",
       "1  At some time in 2020, the Russian social netwo...  \n",
       "2  In 2014, this list of records surfaced online ...  \n",
       "3  In November 2019, the Russian online electroni...  \n",
       "4  At an unconfirmed date, StoreLP.ru's database ...  \n",
       "5  At an unconfirmed date, data purportedly assoc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_ = json.loads(response.text)\n",
    "lol1 = list()\n",
    "lol2 = list()\n",
    "lol3 = list()\n",
    "\n",
    "#response.text\n",
    "ree_ = re_['breaches']\n",
    "for keyss in ree_.keys():\n",
    "    #print(ree_[keyss]['publishDate'])\n",
    "    lol1.append(ree_[keyss]['publishDate'])\n",
    "    #print(ree_[keyss]['site'])\n",
    "    lol2.append(ree_[keyss]['site']) \n",
    "    lol3.append(ree_[keyss]['description'])\n",
    "    #print(ree_[keyss]['description'])\n",
    "    \n",
    "kom_news = pd.DataFrame(lol1)\n",
    "kom_news['site'] = pd.DataFrame(lol2)\n",
    "kom_news['description'] = pd.DataFrame(lol3)\n",
    "kom_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная часть (необязательная)\n",
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.getGROUP = 'netology'\n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
